{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Ladvien/nn_lovecraft/blob/master/lovecraft_ghostwriter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "yfApa1Q9Gpbk",
    "outputId": "87ebe6b1-1966-4dfb-b1d7-303b07e55c78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'nn_lovecraft'...\n",
      "remote: Enumerating objects: 35, done.\u001b[K\n",
      "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
      "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
      "remote: Total 119 (delta 18), reused 19 (delta 7), pack-reused 84\u001b[K\n",
      "Receiving objects: 100% (119/119), 11.47 MiB | 25.36 MiB/s, done.\n",
      "Resolving deltas: 100% (21/21), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Ladvien/nn_lovecraft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQ1MUC3fFhGp"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sat Dec 29 23:49:00 2018\n",
    "\n",
    "@author: ladvien\n",
    "\"\"\"\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "# import tensorflow.contrib.eager as tfe\n",
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vj01WCTSFm_F"
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# References #\n",
    "##############\n",
    "# http://complx.me/2016-12-31-practical-seq2seq/\n",
    "# https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\n",
    "# http://ruder.io/deep-learning-nlp-best-practices/index.html#bestpractices\n",
    "# https://github.com/andreamad8/Universal-Transformer-Pytorch\n",
    "# https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/text/transformer.ipynb?authuser=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OcdUkJM0F0J9"
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# Parameters #\n",
    "##############\n",
    "\n",
    "test_sentence           = '<sos> the darkness my old friend '\n",
    "\n",
    "num_examples            = 5000000\n",
    "retain_threshold        = 15\n",
    "min_perc_sent           = 0.2\n",
    "max_perc_sent           = 0.8\n",
    "corpus_samples          = 10\n",
    "freq_threshold          = 2\n",
    "\n",
    "max_sentence_len        = 200\n",
    "min_sentence_len        = 20\n",
    "\n",
    "epochs                  = 300\n",
    "embedding_dim           = 8196\n",
    "units                   = 1024\n",
    "batch_size              = 16\n",
    "attention_units         = 24\n",
    "decoder_hidden_units    = 256\n",
    "encoder_dropout         = 0.5\n",
    "decoder_dropout         = 0.5\n",
    "steps_per_epoch         = 10000\n",
    "\n",
    "split_sent_on           = r'[.!?]'\n",
    "\n",
    "workpath                = './nn_lovecraft'\n",
    "save_model_path         = './nn_lovecraft/data/models'\n",
    "corpus_path             = workpath + '/data/lovecraft_corpus.txt'\n",
    "output_filepath         = workpath + '/training_samples.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpfyiWATF27t"
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Special Tokens\n",
    "#################\n",
    "\n",
    "start_of_sent           = '<sos>'  \n",
    "end_of_sent             = '<eos>'\n",
    "low_freq_word           = '<lfw>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zEkQvlntF5kl"
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Aid functions                        #\n",
    "########################################\n",
    "\n",
    "def clean_special_chars(text, convert_to_space = [], remove = []):\n",
    "    \n",
    "    ellipses = '<elp>'\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Artifact\n",
    "    text = text.replace('return to table of contents', '')\n",
    "    \n",
    "    # Replace ellipses with token.\n",
    "    text = text.replace('. . .', ellipses)\n",
    "    text = text.replace('. . . .', ellipses)\n",
    "    \n",
    "    # Replaces new lines\n",
    "    text = re.sub('\\n', '', text)\n",
    "    \n",
    "    # Replaces multiple spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    # Replace handidness of quotations.\n",
    "    text = re.sub(r'[“”\"()]', '', text)\n",
    "    \n",
    "    # Opens parantheticals and speed-ups.\n",
    "    text = re.sub('—', ' ', text)\n",
    "    \n",
    "    \n",
    "    punctionation_marks = ['.', ',', '!', '?', ';', ':', '’s']\n",
    "    \n",
    "    for mark in punctionation_marks:\n",
    "        text = text.replace(mark, ' ' + mark + ' ')\n",
    "    \n",
    "    # Replaces multiple spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def commonize_low_freq_words(sentences, word_frequencies, threshold, low_freq_word):\n",
    "    \n",
    "    print('')\n",
    "    print(f'Replacing low-frequency words with {low_freq_word}')\n",
    "    \n",
    "    index = 0\n",
    "    last_perc_comp = 0\n",
    "    \n",
    "    low_freq = []\n",
    "    for key, value in word_frequencies.items():\n",
    "        if value < freq_threshold:\n",
    "            low_freq.append(key)\n",
    "            \n",
    "    clean_sentences = []\n",
    "    for sentence in sentences:\n",
    "        clean_sentence = ''\n",
    "        for word in sentence.split(' '):\n",
    "            if word in low_freq:\n",
    "                clean_sentence += ' ' + low_freq_word\n",
    "            else:\n",
    "                clean_sentence += ' ' + word\n",
    "        clean_sentences.append(clean_sentence)      \n",
    "        index += 1\n",
    "        \n",
    "        perc_comp = int(round((index / len(sentences)) * 100, 2))\n",
    "        if perc_comp % 10 == 0 and last_perc_comp < perc_comp:\n",
    "            print(f'Complete: {str(perc_comp)}%')\n",
    "            last_perc_comp = perc_comp\n",
    "\n",
    "    return clean_sentences\n",
    "    \n",
    "# Get list of distinct words in string\n",
    "def get_words_and_frequencies(text, delimiter = ' '):\n",
    "    words = text.split(delimiter)\n",
    "    word_frequencies = {}\n",
    "    for word in words:\n",
    "        word = re.sub('[^A-Za-z0-9]+', '', word)\n",
    "        word = word.strip()   \n",
    "        if word not in word_frequencies:\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "    return word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iDHdPRuIF56y"
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Load the Corpus                      #\n",
    "########################################\n",
    "with io.open(corpus_path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "-P7bwB3nF7GV",
    "outputId": "b139d603-a16c-40c8-8b63-4bc4847f21ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replacing low-frequency words with <lfw>\n",
      "Complete: 10%\n",
      "Complete: 20%\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# Sentences                            #\n",
    "########################################\n",
    "    \n",
    "# Clear special characters.\n",
    "text = clean_special_chars(text)\n",
    "\n",
    "# Split into sentences by '.', ',', '!', ';', or '?'\n",
    "sentences = re.split(split_sent_on, text)\n",
    "\n",
    "# Limit sentence size\n",
    "\n",
    "new_sentences = []\n",
    "for sentence in sentences:\n",
    "    if len(sentence) < max_sentence_len:\n",
    "        new_sentences.append(sentence)\n",
    "\n",
    "sentences = new_sentences\n",
    "\n",
    "# Limit the samples\n",
    "sentences = sentences[0:num_examples]\n",
    "\n",
    "# Add start and stop tokens.\n",
    "sentences = [start_of_sent + ' ' + text + ' ' + end_of_sent for text in sentences]\n",
    "\n",
    "# Remove blank sentences.\n",
    "for sentence in sentences:\n",
    "    num_words = len(sentence.split(' '))\n",
    "    if num_words <= retain_threshold:\n",
    "        sentences.remove(sentence)\n",
    "\n",
    "# Preseverse import strings.\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = sentences[i].strip()\n",
    "\n",
    "# Get the frequency of words\n",
    "word_freqs = get_words_and_frequencies(text)\n",
    "\n",
    "# Divide the cleaned corpus into sentences\n",
    "sentences = commonize_low_freq_words(sentences, word_freqs, freq_threshold, low_freq_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UVMh6Zv7F-8F"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# Get Sentence Heads and Butts  #\n",
    "#################################\n",
    "heads = []\n",
    "butts = []\n",
    "\n",
    "for _ in range(corpus_samples):\n",
    "    \n",
    "    # Split sentence into words\n",
    "    for sentence in sentences:\n",
    "        sent_word_list = sentence.split(' ')\n",
    "        sentence_len = len(sent_word_list)\n",
    "        \n",
    "        # Get split ratio\n",
    "        split_index = int(sentence_len * random.uniform(min_perc_sent, max_perc_sent))  \n",
    "\n",
    "        # Make sure there are enough words in sentence to create a head and butt.\n",
    "        if sentence_len > split_index and split_index > min_sentence_len:\n",
    "            # Split the sentence at a random index.\n",
    "            heads.append(' '.join(sent_word_list[0:split_index]))\n",
    "            butts.append(' '.join(sent_word_list[split_index:sentence_len + 1]))\n",
    "\n",
    "\n",
    "text = ''\n",
    "for i in range(len(heads)):\n",
    "    text += heads[i]\n",
    "    text += butts[i]\n",
    "    text = text.strip()\n",
    "    text += '. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "9bYF6yAwGB0V",
    "outputId": "222e7b2c-e2f0-4fda-9106-983f4e9e7d41"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# Tokenize Heads and Butts      #\n",
    "#################################\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "\n",
    "# Include model signals in the token set.\n",
    "special_tokens = [start_of_sent, end_of_sent, low_freq_word]\n",
    "sentences  = sentences + special_tokens\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "tokens = tokenizer.word_index\n",
    "\n",
    "# Tokenize the heads and butt lists.\n",
    "tokenized_heads = tokenizer.texts_to_sequences(heads)\n",
    "tokenized_butts = tokenizer.texts_to_sequences(butts)\n",
    "\n",
    "# Pad\n",
    "tokenized_heads = tf.keras.preprocessing.sequence.pad_sequences(tokenized_heads, padding='pre')\n",
    "tokenized_butts = tf.keras.preprocessing.sequence.pad_sequences(tokenized_butts, padding='post')\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "max_length_heads, max_length_butts = max_length(tokenized_heads), max_length(tokenized_butts)\n",
    "\n",
    "if max_length_butts != max_length_heads:\n",
    "    print(\"Max lengths from the input sentences and output do not match.\")\n",
    "\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(tokenized_heads, tokenized_butts, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
    "\n",
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t != 0:\n",
    "      print (f'{t:{5}} ----> {lang.index_word[t]}')\n",
    "\n",
    "print (\"Input Language; index to word mapping\")\n",
    "random_sent = random.randint(0, len(input_tensor_train))\n",
    "convert(tokenizer, input_tensor_train[random_sent])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(tokenizer, target_tensor_train[random_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Fw7dlqIlGEOt",
    "outputId": "059ffc67-2736-437a-86b5-e0d393f3b49a"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# MODEL Setup\n",
    "########################\n",
    "BUFFER_SIZE             = len(input_tensor_train)\n",
    "vocab_inp_size          = len(tokenizer.word_index) + 1\n",
    "vocab_tar_size          = len(tokenizer.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "nRTknMVlGFv_",
    "outputId": "b4ed73c0-5d4e-4bc9-8a15-bf0580b4210e"
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Seq2Seq\n",
    "#################\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout = 0.5):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden, dropout = 0.5):\n",
    "    x = self.embedding(x)\n",
    "    x = self.dropout(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size, encoder_dropout)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # hidden shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # we are doing this to perform addition to calculate the score\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "attention_layer = BahdanauAttention(attention_units)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, hidden, dropout = 0.5):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    self.hidden  = tf.keras.layers.Dense(hidden)\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output, dropout = True):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # dropout\n",
    "    if dropout:\n",
    "        output = self.dropout(output)\n",
    "    \n",
    "    output = self.hidden(output)\n",
    "    \n",
    "    if dropout:\n",
    "        output = self.dropout(output)\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, batch_size, decoder_hidden_units, decoder_dropout)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((batch_size, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TnakiqPGIx_"
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Optimizer\n",
    "#################\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yUVc6V44GKmW"
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Save Best\n",
    "#################\n",
    "checkpoint_dir      = './training_checkpoints'\n",
    "checkpoint_prefix   = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint          = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iP4Q_pKXGOkW"
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Train Functions\n",
    "####################\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index[start_of_sent]] * batch_size, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "  print(type(batch_loss))\n",
    "  return batch_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCL50D71GO_W"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Evalulate Functions\n",
    "######################\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_butts, max_length_heads))\n",
    "    sentence = [x for x in sentence.split(' ') if x != '']\n",
    "    sentence = ' '.join(sentence)\n",
    "    inputs = [tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_heads,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden, dropout = 0.0)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index[start_of_sent]], 0)\n",
    "    \n",
    "    for t in range(max_length_butts):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out,\n",
    "                                                             False)\n",
    "    \n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "    \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    \n",
    "        result += tokenizer.index_word[predicted_id] + ' '\n",
    "    \n",
    "        if tokenizer.index_word[predicted_id] == end_of_sent:\n",
    "            return result, sentence, attention_plot\n",
    "    \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "    \n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def generate(sentence, plot = False):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted: {}'.format(result))\n",
    "    \n",
    "    if plot:\n",
    "        attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "        plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "\n",
    "    return result, sentence\n",
    "\n",
    "def get_random_head(heads):\n",
    "    return heads[random.randint(0, len(heads))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OqUzLZQHGQ2O",
    "outputId": "00fc92d5-fe1f-49bd-a1a8-de3833371b08"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Train\n",
    "######################\n",
    "for epoch in range(epochs):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 5 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "        \n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(round(time.time() - start), 2))\n",
    " \n",
    "  # Test\n",
    "  print('')\n",
    "  print(f'Sample from epoch: {epoch}')\n",
    "  with open(output_filepath, 'w+') as f:\n",
    "      # Save samples to file.\n",
    "      f.write(f'Epoch: {epoch}, loss: {str(batch_loss.numpy())}\\n')\n",
    "      for _ in range(5): \n",
    "          head = get_random_head(heads)\n",
    "          result, sentence = generate(head)\n",
    "          f.write(f'I: {head}\\n')\n",
    "          f.write(f'O:{sentence}\\n')        \n",
    "      result, sentence = generate(test_sentence)\n",
    "      f.write(f'TI: {test_sentence}\\n')\n",
    "      f.write(f'O : {sentence}\\n')\n",
    "      f.write('\\n')\n",
    "  print('')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uDGQ2-_0GSl4"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Evalulate\n",
    "######################\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    \n",
    "\n",
    "# Test 5 heads\n",
    "for _ in range(5):\n",
    "    generate(get_random_head(heads), plot = True)\n",
    "    \n",
    "# Test my own\n",
    "generate(u' <sos> the hound was big ')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "lovecraft_ghostwriter.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
